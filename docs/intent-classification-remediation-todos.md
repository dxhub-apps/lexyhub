# Intent Classification System: Remediation TODO List

**Created:** 2025-11-09
**Related Audit:** `neural-network-buyer-intent-audit.md`
**Priority Levels:** üî¥ Critical | üü° High | üü¢ Medium | üîµ Low

---

## üî¥ CRITICAL: Legal & Compliance (Complete within 2 weeks)

### C1: Update Marketing Claims
**Impact:** Legal risk, false advertising, loss of customer trust
**Owner:** Marketing + Legal

- [ ] **C1.1** Remove or qualify "neural network" claims
  - Replace with: "AI-powered classification using OpenAI GPT-4o-mini"
  - Disclose: "Predictions are generated by third-party AI, not trained on buyer behavior"

- [ ] **C1.2** Remove "learns buyer intent" language
  - Replace with: "Infers intent patterns from keyword semantics"
  - Add disclaimer: "Classifications are predictions, not guarantees"

- [ ] **C1.3** Update website, landing pages, and sales materials
  - Files to update:
    - `docs/business-overview.md` (lines 34, 153-168)
    - Website marketing copy
    - Sales decks and pitch materials

- [ ] **C1.4** Add AI transparency disclosures
  - Add to Terms of Service: "AI classifications are not based on actual purchase data"
  - Add to UI: "‚ö†Ô∏è AI predictions may be inaccurate. Verify important decisions."

**Acceptance Criteria:**
- [ ] Legal review completed
- [ ] No unsubstantiated performance claims remain
- [ ] All "neural network learns buyer intent" language removed

---

### C2: Implement User Consent for Training Data
**Impact:** GDPR/CCPA violations, ‚Ç¨20M fines, user trust damage
**Owner:** Engineering + Legal

- [ ] **C2.1** Add user consent UI
  - Location: User Settings ‚Üí Privacy
  - Text: "Allow LexyHub to use my search queries to improve AI models"
  - Default: **OFF** (opt-in, not opt-out)
  - File: `src/app/(app)/profile/page.tsx`

- [ ] **C2.2** Update database to store consent
  ```sql
  ALTER TABLE user_profiles
  ADD COLUMN ai_training_consent BOOLEAN DEFAULT false;
  ```

- [ ] **C2.3** Update training eligibility check
  - File: `src/lib/rag/training-collector.ts:21-25`
  - Change: Query `user_profiles.ai_training_consent` instead of hardcoded `false`

- [ ] **C2.4** Update Privacy Policy
  - Disclose: Training data collection (currently disabled)
  - Explain: How data is used, stored, and protected
  - Provide: Data export and deletion options (GDPR Article 15, 17)

- [ ] **C2.5** Implement data deletion API
  - Endpoint: `DELETE /api/user/training-data`
  - Action: Hard delete from `lexybrain_requests`, `lexybrain_responses`, `lexybrain_feedback`
  - File: Create `src/app/api/user/training-data/route.ts`

**Acceptance Criteria:**
- [ ] GDPR legal review passed
- [ ] Consent UI live in production
- [ ] Users can opt-in/opt-out
- [ ] Users can export and delete their training data

---

### C3: Add UI Disclaimers for Intent Classifications
**Impact:** User trust, liability for bad decisions based on AI predictions
**Owner:** Product + Engineering

- [ ] **C3.1** Add disclaimer to Intent Graph
  - File: `src/components/insights/IntentGraph.tsx`
  - Location: Above graph visualization
  - Text:
    ```
    ‚ö†Ô∏è Intent classifications are AI predictions based on keyword text only,
    not actual buyer behavior. Use as a research guide, not a guarantee.
    Confidence scores are not validated against real purchase data.
    ```

- [ ] **C3.2** Add tooltip to confidence scores
  - Hover text: "Confidence represents AI certainty, not validated accuracy. Treat all predictions as suggestions."

- [ ] **C3.3** Add "Learn More" link
  - Link to: Help Center article explaining intent classification methodology
  - Article to create: "How LexyHub Intent Classification Works" (methodology transparency)

**Acceptance Criteria:**
- [ ] Disclaimers visible on all intent-related UI components
- [ ] Users cannot miss warnings before making decisions
- [ ] Help documentation published

---

## üü° HIGH PRIORITY: Validation & Measurement (Complete within 1 month)

### H1: Establish Ground-Truth Validation Dataset
**Impact:** Prove (or disprove) AI accuracy, guide improvements
**Owner:** Data Science + Product

- [ ] **H1.1** Collect 1,000 keyword samples
  - Source: Real user searches from production database
  - Stratify by:
    - Marketplace (Etsy, Amazon if available)
    - Category (jewelry, home decor, digital, etc.)
    - Keyword length (short-tail vs. long-tail)

- [ ] **H1.2** Manual labeling by domain experts
  - Hire 3 e-commerce experts (ex-Etsy sellers, Amazon FBA consultants)
  - Labeling instructions:
    - Intent: discovery, research, education, purchase, wholesale
    - Purchase Stage: awareness, consideration, purchase
    - Persona: trend researcher, ready-to-buy shopper, DIY maker, reseller, analyst
  - Inter-rater agreement target: Cohen's Kappa > 0.7

- [ ] **H1.3** Store labels in database
  ```sql
  CREATE TABLE intent_ground_truth (
    id UUID PRIMARY KEY,
    keyword_text TEXT NOT NULL,
    market TEXT NOT NULL,
    true_intent TEXT NOT NULL,
    true_purchase_stage TEXT NOT NULL,
    true_persona TEXT NOT NULL,
    labeler_id UUID,
    confidence TEXT CHECK (confidence IN ('certain', 'probable', 'uncertain')),
    notes TEXT,
    created_at TIMESTAMPTZ DEFAULT now()
  );
  ```

- [ ] **H1.4** Split dataset
  - Validation set: 500 keywords (for accuracy measurement)
  - Test set: 500 keywords (for final evaluation, kept hidden)

**Acceptance Criteria:**
- [ ] 1,000 keywords labeled by 3 independent raters
- [ ] Inter-rater agreement ‚â• 0.7 (Cohen's Kappa)
- [ ] Ground-truth database table populated

---

### H2: Measure Baseline Accuracy
**Impact:** Know current performance, set improvement targets
**Owner:** Data Science

- [ ] **H2.1** Run current system on validation set
  - Loop through 500 ground-truth keywords
  - Collect GPT-4o-mini predictions
  - Store in `ai_predictions` table with `evaluation_run_id`

- [ ] **H2.2** Calculate accuracy metrics
  - Overall accuracy: % of correct intent predictions
  - Per-class F1 score: For each intent type (discovery, purchase, etc.)
  - Precision and recall: For high-stakes "purchase" intent
  - Confusion matrix: Which intents are confused with each other?
  - Confidence calibration: Do 0.65 confidence predictions actually achieve 65% accuracy?

- [ ] **H2.3** Implement evaluation script
  - File: Create `scripts/evaluate-intent-classifier.ts`
  - Libraries: Use `sklearn.metrics` (via Python subprocess) or implement in TypeScript
  - Output: JSON report with all metrics

- [ ] **H2.4** Document baseline results
  - File: Create `docs/intent-classification-baseline.md`
  - Include: Accuracy, F1, confusion matrix, error analysis
  - Example findings: "GPT-4o-mini achieves 62% accuracy, confuses 'research' with 'discovery' 30% of the time"

**Acceptance Criteria:**
- [ ] Baseline accuracy report published
- [ ] Metrics tracked in dashboard (PostHog or internal analytics)
- [ ] Leadership acknowledges current performance level

**Target Metrics (Example):**
- Overall accuracy: ‚â• 75%
- Purchase intent precision: ‚â• 85% (high-stakes classification)
- Discovery intent recall: ‚â• 70%
- Confidence calibration: Within ¬±10% of true accuracy

---

### H3: Implement A/B Testing Infrastructure
**Impact:** Prove intent classification provides user value
**Owner:** Engineering + Data Science

- [ ] **H3.1** Add experiment framework
  - Library: Use LaunchDarkly, Optimizely, or build custom with PostHog feature flags
  - Capabilities:
    - User cohort assignment (50% control, 50% treatment)
    - Metric tracking per cohort
    - Statistical significance testing

- [ ] **H3.2** Design experiment: Intent-Aware Keyword Ranking
  - **Control Group:** Keyword search results sorted by demand index only
  - **Treatment Group:** Keyword search results boosted by intent score
    - Formula: `combined_score = demand_index * 0.7 + purchase_intent_weight * 0.3`
  - **Hypothesis:** Users find profitable keywords faster with intent weighting

- [ ] **H3.3** Define success metrics
  - Primary: % of users who add keywords to watchlist (proxy for value)
  - Secondary: Time to first watchlist add (speed of discovery)
  - Tertiary: User retention at 7 days, 30 days

- [ ] **H3.4** Run experiment for 2 weeks
  - Sample size: 1,000 users (500 control, 500 treatment)
  - Monitoring: Daily check for anomalies, user complaints

- [ ] **H3.5** Analyze results
  - Statistical test: Two-proportion z-test or chi-square
  - Significance level: p < 0.05
  - Effect size: Cohen's h or odds ratio
  - Decision: Ship if treatment wins, rollback if control wins

**Acceptance Criteria:**
- [ ] Experiment framework deployed
- [ ] At least 1 A/B test completed
- [ ] Data-driven decision made (ship or don't ship intent ranking)

---

### H4: Validate Cross-Marketplace Generalization
**Impact:** Ensure Etsy prompts work on Amazon, eBay, etc.
**Owner:** Data Science

- [ ] **H4.1** Collect Amazon keyword samples
  - Source: Amazon autocomplete API, user submissions, or purchase dataset
  - Size: 200 keywords across categories (electronics, books, home, fashion)

- [ ] **H4.2** Manually label Amazon samples
  - Same labeling protocol as H1.2
  - Check for domain-specific differences (e.g., Amazon Prime Day keywords)

- [ ] **H4.3** Run intent classifier on Amazon samples
  - Evaluate: Does GPT-4o-mini perform well without Etsy-specific prompts?
  - Metrics: Accuracy, F1 per class

- [ ] **H4.4** Error analysis
  - Identify: Keywords misclassified on Amazon but not Etsy
  - Hypothesis: E.g., "electronics review" misclassified as "education" instead of "research"

- [ ] **H4.5** Adjust prompts for multi-marketplace
  - Add marketplace-specific examples to system prompt
  - Test again and measure improvement

**Acceptance Criteria:**
- [ ] Amazon validation accuracy ‚â• 70% (within 5-10% of Etsy baseline)
- [ ] Prompt updates published if needed
- [ ] Cross-marketplace performance documented

---

## üü¢ MEDIUM PRIORITY: Product Integration (Complete within 2 months)

### M1: Integrate Intent into Keyword Ranking
**Impact:** Make intent classification useful, not just decorative
**Owner:** Product + Engineering

- [ ] **M1.1** Add intent scoring to keyword API
  - File: `src/app/api/keywords/search/route.ts` (or similar)
  - Logic:
    ```typescript
    if (keyword.extras?.classification?.intent === 'purchase') {
      keyword.opportunity_boost = 1.2; // 20% boost for high-intent
    }
    ```

- [ ] **M1.2** Update Keyword Intelligence Dashboard
  - Add filter: "Show only high-intent keywords" (purchase + wholesale)
  - Add sort option: "Intent score" (descending from purchase ‚Üí discovery)

- [ ] **M1.3** Add intent to Market Twin recommendations
  - File: `src/lib/market-twin/simulator.ts`
  - Enhancement: Suggest high-intent tags in scenario optimization

- [ ] **M1.4** A/B test ranking changes (per H3 experiment)

**Acceptance Criteria:**
- [ ] Intent score influences keyword ranking
- [ ] Users can filter by intent type
- [ ] A/B test proves positive impact (or feature is rolled back)

---

### M2: Add User Feedback Loop
**Impact:** Improve accuracy over time via active learning
**Owner:** Product + Engineering

- [ ] **M2.1** Add "Report Incorrect Intent" button
  - Location: Intent Graph, Keyword detail pages
  - UI: Thumbs up/down with optional text feedback

- [ ] **M2.2** Store feedback in database
  - Table: `intent_user_feedback`
  - Fields: keyword_id, user_id, predicted_intent, user_suggested_intent, notes, created_at

- [ ] **M2.3** Build admin review dashboard
  - Location: `/app/admin/backoffice/intent-feedback`
  - Features:
    - View all user corrections
    - Accept/reject suggestions
    - Bulk update incorrect classifications

- [ ] **M2.4** Use feedback to improve prompts
  - Analyze: Common error patterns (e.g., "research" vs. "discovery" confusion)
  - Update: System prompt with better examples
  - Re-evaluate: Measure accuracy improvement on validation set

**Acceptance Criteria:**
- [ ] Users can submit intent corrections
- [ ] Admin dashboard shows feedback queue
- [ ] At least 100 user corrections collected in first month
- [ ] Prompt improvements deployed and validated

---

### M3: Add Confidence Thresholds
**Impact:** Don't show low-confidence predictions to users
**Owner:** Engineering

- [ ] **M3.1** Analyze confidence distribution
  - Query: `SELECT confidence, COUNT(*) FROM keywords WHERE extras->>'classification' IS NOT NULL GROUP BY confidence`
  - Determine: What confidence level corresponds to acceptable accuracy?

- [ ] **M3.2** Set threshold based on validation data
  - Example: If accuracy ‚â• 75% only when confidence ‚â• 0.6, reject predictions below 0.6

- [ ] **M3.3** Update UI to hide low-confidence predictions
  - File: `src/components/insights/IntentGraph.tsx`
  - Logic:
    ```typescript
    if (node.classification?.confidence < 0.6) {
      node.intent = 'unclassified';
      node.intentNote = 'Intent uncertain - more data needed';
    }
    ```

- [ ] **M3.4** Add "Request Manual Review" for uncertain keywords
  - Allow users to flag keywords for expert classification
  - Store in admin queue for human labeling

**Acceptance Criteria:**
- [ ] Low-confidence predictions hidden or clearly marked
- [ ] Accuracy of shown predictions improves (fewer user complaints)
- [ ] Manual review queue populated

---

### M4: Build Intent Classification Explainability
**Impact:** Users understand WHY intent was classified
**Owner:** Engineering + UX

- [ ] **M4.1** Extract explanation from GPT-4o-mini
  - Update prompt to request: `"reasoning": "Why this keyword indicates purchase intent"`
  - Store in: `keywords.extras.classification.reasoning`

- [ ] **M4.2** Display reasoning in UI
  - Location: Intent Graph node tooltip, Keyword detail page
  - Example:
    ```
    Intent: Purchase
    Reasoning: Keyword contains "buy now" and "price", indicating transactional intent.
    Confidence: 0.72
    ```

- [ ] **M4.3** Highlight keyword features
  - Bold words in keyword text that influenced classification
  - Example: "**buy** handmade **candles** online" (bold = classification signals)

- [ ] **M4.4** Show similar classified keywords
  - "Other 'purchase intent' keywords: buy soap, handmade jewelry for sale"

**Acceptance Criteria:**
- [ ] Users can see why intent was assigned
- [ ] Reasoning is human-readable and plausible
- [ ] Transparency increases user trust (measured via surveys)

---

## üîµ LOW PRIORITY: Advanced ML Capabilities (Complete within 6 months)

### L1: Fine-Tune Custom Intent Classification Model
**Impact:** Improve accuracy beyond zero-shot GPT
**Owner:** ML Engineering (hire if needed)

- [ ] **L1.1** Collect 10,000 labeled keywords
  - Use: Ground-truth dataset (H1) + user feedback (M2) + purchased labeled dataset
  - Source: Mechanical Turk, Labelbox, or internal labeling team

- [ ] **L1.2** Fine-tune OpenAI model
  - Endpoint: `POST https://api.openai.com/v1/fine_tuning/jobs`
  - Model: `gpt-4o-mini` or `gpt-3.5-turbo` (cheaper)
  - Training: Upload JSONL with prompt-completion pairs
  - Cost estimate: ~$200-$500 for 10K examples

- [ ] **L1.3** Evaluate fine-tuned model
  - Compare: Fine-tuned vs. zero-shot on test set (500 keywords)
  - Target: ‚â• 10% accuracy improvement (e.g., 62% ‚Üí 72%)

- [ ] **L1.4** Deploy fine-tuned model
  - Update: `src/lib/ai/intent-classifier.ts:99` to use custom model ID
  - Monitor: Latency, cost, accuracy in production

- [ ] **L1.5** Set up retraining pipeline
  - Frequency: Quarterly (every 3 months)
  - Trigger: When new labeled data reaches 2,500 samples
  - Automation: GitHub Action or cron job to retrain and evaluate

**Acceptance Criteria:**
- [ ] Fine-tuned model deployed to production
- [ ] Accuracy improvement ‚â• 10% on test set
- [ ] Retraining pipeline automated

**Cost Estimate:**
- Initial fine-tuning: $200-$500
- Quarterly retraining: $100-$200
- API cost increase: ~$50-$100/month (fine-tuned models are slightly more expensive)

---

### L2: Integrate Real Behavioral Data
**Impact:** Learn intent from actual buyer actions, not just keyword text
**Owner:** Data Engineering + Partnerships

- [ ] **L2.1** Partner with Etsy/Amazon sellers for conversion data
  - Approach: Offer free premium tier in exchange for anonymized sales data
  - Data needed: `{keyword_searched, listing_viewed, purchase_made (yes/no)}`

- [ ] **L2.2** Build conversion tracking API
  - Endpoint: `POST /api/analytics/conversion`
  - Payload: `{user_id (anonymized), keyword, action: 'view' | 'add_to_cart' | 'purchase'}`
  - Storage: New table `keyword_conversions`

- [ ] **L2.3** Calculate intent from conversion rate
  - Logic:
    ```sql
    SELECT
      keyword,
      COUNT(*) FILTER (WHERE action = 'purchase') * 100.0 / COUNT(*) AS purchase_intent_score
    FROM keyword_conversions
    GROUP BY keyword;
    ```
  - High purchase rate ‚Üí purchase intent
  - High view rate, low purchase ‚Üí research intent

- [ ] **L2.4** Hybrid model: GPT + behavioral data
  - Combine: GPT-4o-mini prediction (60% weight) + conversion data (40% weight)
  - Formula:
    ```
    final_intent_score = 0.6 * gpt_confidence + 0.4 * conversion_rate
    ```

- [ ] **L2.5** Evaluate hybrid model on test set
  - Target: ‚â• 15% accuracy improvement over GPT alone

**Acceptance Criteria:**
- [ ] At least 50,000 conversion events collected
- [ ] Hybrid model deployed
- [ ] Accuracy improves significantly

**Legal/Privacy Requirements:**
- [ ] User consent for conversion tracking
- [ ] Data anonymization (no PII)
- [ ] GDPR-compliant data retention (auto-delete after 90 days)

---

### L3: Multi-Marketplace Model Specialization
**Impact:** Optimize performance per marketplace
**Owner:** ML Engineering

- [ ] **L3.1** Train marketplace-specific models
  - Etsy model: Fine-tuned on Etsy keywords (handmade, vintage, craft supplies)
  - Amazon model: Fine-tuned on Amazon keywords (electronics, books, prime)
  - eBay model: Fine-tuned on eBay keywords (auctions, used goods, collectibles)

- [ ] **L3.2** Model router based on marketplace
  - File: `src/lib/ai/intent-classifier.ts`
  - Logic:
    ```typescript
    const modelMap = {
      etsy: 'ft:gpt-4o-mini:lexyhub:etsy-v1',
      amazon: 'ft:gpt-4o-mini:lexyhub:amazon-v1',
      ebay: 'ft:gpt-4o-mini:lexyhub:ebay-v1',
    };
    const model = modelMap[input.market] || 'gpt-4o-mini'; // fallback
    ```

- [ ] **L3.3** Evaluate per-marketplace accuracy
  - Target: Each marketplace model ‚â• 80% accuracy on its domain

- [ ] **L3.4** Cost optimization
  - Use smaller models (GPT-3.5-turbo) for less critical classifications
  - Cache predictions for 30 days to reduce API calls

**Acceptance Criteria:**
- [ ] Marketplace-specific models deployed
- [ ] Accuracy ‚â• 80% on each marketplace
- [ ] API costs controlled (< $500/month)

---

### L4: Build Custom Embedding Model
**Impact:** Semantic search optimized for e-commerce, not general text
**Owner:** ML Engineering (requires ML expertise)

- [ ] **L4.1** Collect e-commerce keyword pairs
  - Positive pairs (similar): "handmade candles" ‚Üî "artisan wax melts"
  - Negative pairs (dissimilar): "handmade candles" ‚Üî "laptop charger"
  - Size: 50,000 pairs

- [ ] **L4.2** Fine-tune sentence-transformers model
  - Base model: `all-MiniLM-L6-v2` or `e5-large`
  - Loss: Contrastive loss or triplet loss
  - Training: 10 epochs, batch size 32
  - Infrastructure: AWS SageMaker or Google Colab with GPU

- [ ] **L4.3** Evaluate embedding quality
  - Metric: Retrieval accuracy (% of similar keywords in top-10 nearest neighbors)
  - Target: ‚â• 85% retrieval accuracy (vs. 70% for OpenAI general embeddings)

- [ ] **L4.4** Deploy custom embedding service
  - Host on: AWS Lambda (inference) or dedicated GPU server
  - Latency target: < 100ms per keyword

- [ ] **L4.5** Replace OpenAI embeddings gradually
  - Rollout: 10% ‚Üí 50% ‚Üí 100% of keywords
  - Monitor: Embedding quality, API cost savings (~70% cheaper than OpenAI)

**Acceptance Criteria:**
- [ ] Custom embedding model deployed
- [ ] Retrieval accuracy ‚â• 85%
- [ ] API costs reduced by ‚â• 50%

**Cost Estimate:**
- Training: $100-$300 (GPU hours)
- Hosting: $100-$200/month (inference server)
- Savings: ~$300-$500/month (vs. OpenAI API)

---

### L5: Intent Drift Monitoring
**Impact:** Detect when model accuracy degrades over time
**Owner:** ML Engineering + DevOps

- [ ] **L5.1** Set up accuracy monitoring
  - Weekly: Run classifier on 100-sample holdout set (fixed test set)
  - Metric: Track accuracy, F1, precision over time
  - Alert: If accuracy drops > 5% from baseline

- [ ] **L5.2** Build monitoring dashboard
  - Tool: Grafana, Datadog, or custom dashboard
  - Charts:
    - Intent classification accuracy over time
    - Confidence score distribution per week
    - Classification latency (p50, p95, p99)
    - API cost per 1,000 classifications

- [ ] **L5.3** Root cause analysis playbook
  - If accuracy drops, investigate:
    - OpenAI model update (did GPT-4o-mini change?)
    - Data drift (new keyword types from users)
    - Prompt staleness (are examples outdated?)

- [ ] **L5.4** Automated retraining trigger
  - If accuracy drops > 10%: Auto-trigger retraining job
  - Slack alert: "@ml-team Intent classifier accuracy dropped to 68% (baseline 78%). Retraining initiated."

**Acceptance Criteria:**
- [ ] Monitoring dashboard live
- [ ] Alerts configured for accuracy drops
- [ ] Incident response playbook documented

---

## üéØ Success Metrics & KPIs

### Before Remediation (Baseline)
- ‚ùå **Accuracy:** Unknown (not measured)
- ‚ùå **User Value:** Unknown (no A/B tests)
- ‚ùå **Transparency:** 0% (no disclaimers)
- ‚ùå **Compliance:** At risk (no consent mechanism)

### After Remediation (Target)
- ‚úÖ **Accuracy:** ‚â• 75% on validation set
- ‚úÖ **User Value:** +10% watchlist add rate (A/B test winner)
- ‚úÖ **Transparency:** 100% (disclaimers on all UI)
- ‚úÖ **Compliance:** Full GDPR/CCPA compliance

### Quarterly Review Metrics
1. **Model Performance:**
   - Overall accuracy (target: ‚â• 75%)
   - Purchase intent precision (target: ‚â• 85%)
   - Confidence calibration error (target: < 10%)

2. **User Engagement:**
   - % of users who interact with intent features (target: ‚â• 30%)
   - User feedback submissions per month (target: ‚â• 100)
   - Intent-influenced keyword searches (target: ‚â• 20% of searches)

3. **Business Impact:**
   - Seller revenue attributed to intent recommendations (requires conversion tracking)
   - User retention for intent feature users vs. non-users
   - Premium plan conversions (do intent features drive upgrades?)

4. **Technical Health:**
   - API cost per 1,000 classifications (target: < $0.50)
   - Classification latency p95 (target: < 2 seconds)
   - Model accuracy drift (target: < 5% drop per quarter)

---

## üìÖ Implementation Timeline

### Phase 1: Critical Compliance (Weeks 1-2)
- Complete all **üî¥ CRITICAL** tasks (C1-C3)
- Legal review and approval
- Deploy disclaimers and consent UI

### Phase 2: Validation Foundation (Weeks 3-6)
- Complete **üü° HIGH PRIORITY** tasks (H1-H4)
- Establish ground-truth dataset
- Measure baseline accuracy
- Run first A/B test

### Phase 3: Product Integration (Weeks 7-14)
- Complete **üü¢ MEDIUM PRIORITY** tasks (M1-M4)
- Integrate intent into ranking
- Build user feedback loop
- Add explainability

### Phase 4: Advanced ML (Months 4-6)
- Begin **üîµ LOW PRIORITY** tasks (L1-L5)
- Fine-tune custom models
- Integrate behavioral data
- Set up monitoring

---

## üö® Risks & Mitigation

### Risk 1: Ground-Truth Labeling is Expensive/Slow
**Mitigation:**
- Start with 500 samples (not 1,000) for faster iteration
- Use user feedback (M2) to supplement manual labeling
- Purchase existing labeled dataset if available

### Risk 2: Fine-Tuning Doesn't Improve Accuracy
**Mitigation:**
- Establish baseline (H2) before investing in fine-tuning
- Use prompt engineering improvements as alternative (cheaper, faster)
- Only fine-tune if zero-shot accuracy < 70%

### Risk 3: Users Don't Engage with Intent Features
**Mitigation:**
- Run A/B test (H3) before deep integration
- If test fails, pivot: Use intent internally (ranking) but don't expose to users
- Deprioritize or sunset feature if no value proven

### Risk 4: GDPR Violation Fines
**Mitigation:**
- Prioritize C2 (user consent) immediately
- Do NOT enable training data collection until consent is live
- Legal review before any deployment

---

## üìã Checklist for Project Manager

- [ ] All **üî¥ CRITICAL** tasks assigned to owners
- [ ] Legal team engaged for C1-C2 review
- [ ] Budget approved for labeling (H1: ~$5K) and fine-tuning (L1: ~$500)
- [ ] Weekly standup scheduled for remediation progress
- [ ] Success metrics dashboard configured
- [ ] Quarterly review scheduled (6 months from now)

---

**Document Owner:** Engineering Leadership
**Last Updated:** 2025-11-09
**Next Review:** 2025-12-09 (1 month check-in)
